import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import argparse # Import module argparse
from evaluation import batch_multi_class_metrics, overall_pixel_accuracy
from model.unet import UNet # Gi·∫£ ƒë·ªãnh UNet ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü ƒë√¢y
from lowlight_dataset import NightCitySegmentationDataset, PairedTransform, ID_MAPPING, IGNORE_INDEX


# ===============================================
# A. H√ÄM X·ª¨ L√ù THAM S·ªê D√íNG L·ªÜNH
# ===============================================
def parse_args():
    parser = argparse.ArgumentParser(description="Hu·∫•n luy·ªán m√¥ h√¨nh Segmentation UNet cho ·∫£nh thi·∫øu s√°ng.")
    
    # THAY TH·∫æ rootdir, imgsubdir, masksubdir b·∫±ng ƒë∆∞·ªùng d·∫´n tr·ª±c ti·∫øp
    parser.add_argument('--imgdir', type=str, required=True,
                        help='ƒê∆∞·ªùng d·∫´n TR·ª∞C TI·∫æP ƒë·∫øn th∆∞ m·ª•c ch·ª©a ·∫£nh hu·∫•n luy·ªán (v√≠ d·ª•: .../images/train)')
    parser.add_argument('--maskdir', type=str, required=True,
                        help='ƒê∆∞·ªùng d·∫´n TR·ª∞C TI·∫æP ƒë·∫øn th∆∞ m·ª•c ch·ª©a mask nh√£n (v√≠ d·ª•: .../labels/train)')
    
    # --- Tham s·ªë B·∫Øt bu·ªôc / Quan tr·ªçng ---
    parser.add_argument('--size', type=int, default=224,
                        help='K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o (N x N) sau khi resize. M·∫∑c ƒë·ªãnh: 224')
    parser.add_argument('--epochs', type=int, default=200,
                        help='T·ªïng s·ªë epoch hu·∫•n luy·ªán. M·∫∑c ƒë·ªãnh: 200')
    parser.add_argument('--batchsize', type=int, default=8,
                        help='K√≠ch th∆∞·ªõc batch. M·∫∑c ƒë·ªãnh: 8')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='T·ªëc ƒë·ªô h·ªçc (Learning Rate) ban ƒë·∫ßu. M·∫∑c ƒë·ªãnh: 1e-4')
    parser.add_argument('--numclasses', type=int, default=20,
                        help='S·ªë l∆∞·ª£ng l·ªõp nh√£n (sau khi √°nh x·∫°). M·∫∑c ƒë·ªãnh: 20')
                        
    # --- Tham s·ªë T√πy ch·ªçn ---
    parser.add_argument('--numworkers', type=int, default=4,
                        help='S·ªë l∆∞·ª£ng worker cho DataLoader. M·∫∑c ƒë·ªãnh: 4')
                        
    return parser.parse_args()


# ===============================================
# B. H√ÄM HU·∫§N LUY·ªÜN CH√çNH
# ===============================================

def train_model(args):
    
    # G√°n tham s·ªë t·ª´ args
    IMG_DIR = args.imgdir
    MASK_DIR = args.maskdir
    TARGET_SIZE = args.size
    NUM_CLASSES = args.numclasses
    BATCH_SIZE = args.batchsize
    NUM_EPOCHS = args.epochs
    LEARNING_RATE = args.lr
    NUM_WORKERS = args.numworkers
    
    # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n TR·ª∞C TI·∫æP
    if not os.path.exists(IMG_DIR):
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ·∫£nh t·∫°i {IMG_DIR}")
        return
    if not os.path.exists(MASK_DIR):
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c nh√£n t·∫°i {MASK_DIR}")
        return
        
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # üåü TH√äM LOGIC IN TH√îNG TIN L·ªöP T·∫†I ƒê√ÇY
    num_trainable_classes = len(ID_MAPPING)
    
    print("="*50)
    print("üìã C·∫§U H√åNH L·ªöP H·ªåC (TRAINING CLASS CONFIG) üìã")
    print("-" * 50)
    print(f"‚úÖ S·ªë l∆∞·ª£ng l·ªõp ƒë∆∞·ª£c hu·∫•n luy·ªán (Train ID: 0 ƒë·∫øn {num_trainable_classes-1}): {num_trainable_classes}")
    print(f"‚ùå L·ªõp B·ªé QUA (IGNORE_INDEX): {IGNORE_INDEX}")
    # 1. Kh·ªüi t·∫°o M√¥ h√¨nh
    model = UNet(n_channels=3, n_classes=NUM_CLASSES).to(device) 
    
    # 2. Kh·ªüi t·∫°o Dataset v√† DataLoader
    data_transform = PairedTransform(size=(TARGET_SIZE, TARGET_SIZE)) 
    
    # Truy·ªÅn ƒë∆∞·ªùng d·∫´n ·∫£nh v√† nh√£n TR·ª∞C TI·∫æP
    train_dataset = NightCitySegmentationDataset(
        img_dir=IMG_DIR,
        mask_dir=MASK_DIR,
        transform=data_transform
    )
    
    print(f"K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o: {TARGET_SIZE}x{TARGET_SIZE} | S·ªë l∆∞·ª£ng m·∫´u: {len(train_dataset)}")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=True
    )

    # 3. Kh·ªüi t·∫°o Loss v√† Optimizer
    IGNORE_INDEX = 255
    criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX).to(device) 
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)
    
    print(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Segmentation tr√™n {device}. T·ªïng s·ªë Epoch: {NUM_EPOCHS}")

    # 4. V√≤ng l·∫∑p Hu·∫•n luy·ªán Ch√≠nh (Gi·ªØ nguy√™n logic t√≠nh to√°n Loss v√† Metrics)
    for epoch in range(NUM_EPOCHS):
        model.train()
        running_loss = 0.0
        
        # Kh·ªüi t·∫°o bi·∫øn t√≠ch l≈©y
        total_mIoU, total_mDice, total_mAcc, total_overall_acc = 0.0, 0.0, 0.0, 0.0
        num_batches = len(train_loader)
    
        for batch_idx, (night_image, target_mask) in enumerate(train_loader):
            
            night_image = night_image.to(device)
            target_mask = target_mask.to(device)
    
            optimizer.zero_grad()
            prediction_logits = model(night_image)
            
            L_Task = criterion(prediction_logits, target_mask)
    
            L_Task.backward()
            optimizer.step()
    
            running_loss += L_Task.item()
            
            # === T√≠nh to√°n Metrics ===
            with torch.no_grad():
                metrics = batch_multi_class_metrics(
                    prediction_logits, 
                    target_mask, 
                    num_classes=NUM_CLASSES
                )
                
                # üåü T√≠ch l≈©y c√°c metrics t·ª´ h√†m m·ªõi
                total_mIoU += metrics['mIoU']
                total_mDice += metrics['mDice']
                total_mAcc += metrics['mAcc']
                # ----------------------------------
                
                # T√≠nh v√† t√≠ch l≈©y Overall Acc (gi·ªØ nguy√™n logic c≈©)
                overall_acc = overall_pixel_accuracy(prediction_logits, target_mask)
                total_overall_acc += overall_acc
    
    
        scheduler.step()
    
        avg_loss = running_loss / num_batches
        # T√≠nh gi√° tr·ªã trung b√¨nh cu·ªëi c√πng c·ªßa c√°c metrics
        avg_mIoU = total_mIoU / num_batches
        avg_mDice = total_mDice / num_batches
        avg_mAcc = total_mAcc / num_batches
        avg_overall_acc = total_overall_acc / num_batches
        
        # === In k·∫øt qu·∫£ Epoch ===
        print(f"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f} | mIoU: {avg_mIoU:.4f} | mDice: {avg_mDice:.4f} | mAcc: {avg_mAcc:.4f} | Overall Acc: {avg_overall_acc:.4f}")
    
        # T√πy ch·ªçn: L∆∞u checkpoint
        if (epoch + 1) % 10 == 0:
            os.makedirs('./checkpoints', exist_ok=True)
            torch.save(
                {'model': model.state_dict()}, 
                f'./checkpoints/checkpoint_epoch_{epoch+1}.pth'
            )
if __name__ == '__main__':
    args = parse_args()
    train_model(args)
